"""
í†µê³„ ë¶„ì„ ì—”ì§„

ì‘ì—… 3: Backend: Develop Core Statistical Analysis Engine
ë‘ í…ŒìŠ¤íŠ¸ ê¸°ê°„('n'ê³¼ 'n-1') ê°„ì˜ í†µê³„ì  ë¹„êµë¥¼ ìˆ˜í–‰í•˜ëŠ” ì—”ì§„ì„ êµ¬í˜„í•©ë‹ˆë‹¤.
"""

import logging
import numpy as np
import pandas as pd
from typing import List, Dict, Any, Tuple, Optional, Union
from datetime import datetime
from dataclasses import dataclass
from enum import Enum
import scipy.stats as stats
from scipy.stats import shapiro, levene, ttest_ind, mannwhitneyu, wilcoxon

logger = logging.getLogger(__name__)

class TestType(Enum):
    """í†µê³„ ê²€ì • ìœ í˜•"""
    T_TEST = "t_test"
    MANN_WHITNEY = "mann_whitney"
    WILCOXON = "wilcoxon"
    PAIRED_T_TEST = "paired_t_test"
    KRUSKAL_WALLIS = "kruskal_wallis"
    ANOVA = "anova"
    KS_TEST = "ks_test"

class EffectSizeType(Enum):
    """íš¨ê³¼ í¬ê¸° ìœ í˜•"""
    COHENS_D = "cohens_d"
    HEDGES_G = "hedges_g"
    CLIFFS_DELTA = "cliffs_delta"

@dataclass
class StatisticalTestResult:
    """í†µê³„ ê²€ì • ê²°ê³¼"""
    test_type: TestType
    statistic: float
    p_value: float
    is_significant: bool
    alpha: float = 0.05
    
    def __post_init__(self):
        self.is_significant = self.p_value < self.alpha

@dataclass
class EffectSizeResult:
    """íš¨ê³¼ í¬ê¸° ê²°ê³¼"""
    effect_size_type: EffectSizeType
    value: float
    interpretation: str
    magnitude: str  # small, medium, large

@dataclass
class MetricAnalysisResult:
    """ë©”íŠ¸ë¦­ë³„ ë¶„ì„ ê²°ê³¼"""
    metric_name: str
    period_n_data: np.ndarray
    period_n1_data: np.ndarray
    test_result: StatisticalTestResult
    effect_size: EffectSizeResult
    clinical_significance: bool
    summary: str

@dataclass
class ComprehensiveAnalysisResult:
    """ì¢…í•© ë¶„ì„ ê²°ê³¼"""
    analysis_id: str
    period_n_info: Dict[str, Any]
    period_n1_info: Dict[str, Any]
    metrics_results: List[MetricAnalysisResult]
    overall_assessment: str
    confidence_level: float
    timestamp: datetime
    total_metrics: int
    significant_metrics: int
    clinically_significant_metrics: int

@dataclass
class IntegratedAnalysisReport:
    """í†µí•© ë¶„ì„ ë³´ê³ ì„œ"""
    report_id: str
    analysis_result: ComprehensiveAnalysisResult
    summary_statistics: Dict[str, Any]
    pass_fail_assessment: Dict[str, Any]
    recommendations: List[str]
    risk_assessment: Dict[str, Any]
    generated_at: datetime
    
    def to_json(self) -> Dict[str, Any]:
        """JSON í˜•íƒœë¡œ ë³€í™˜"""
        return {
            "report_id": self.report_id,
            "generated_at": self.generated_at.isoformat(),
            "summary_statistics": self.summary_statistics,
            "pass_fail_assessment": self.pass_fail_assessment,
            "recommendations": self.recommendations,
            "risk_assessment": self.risk_assessment,
            "analysis_result": {
                "analysis_id": self.analysis_result.analysis_id,
                "period_n_info": self.analysis_result.period_n_info,
                "period_n1_info": self.analysis_result.period_n1_info,
                "overall_assessment": self.analysis_result.overall_assessment,
                "confidence_level": self.analysis_result.confidence_level,
                "timestamp": self.analysis_result.timestamp.isoformat(),
                "total_metrics": self.analysis_result.total_metrics,
                "significant_metrics": self.analysis_result.significant_metrics,
                "clinically_significant_metrics": self.analysis_result.clinically_significant_metrics,
                "metrics_results": [
                    {
                        "metric_name": result.metric_name,
                        "test_result": {
                            "test_type": result.test_result.test_type.value,
                            "statistic": result.test_result.statistic,
                            "p_value": result.test_result.p_value,
                            "is_significant": result.test_result.is_significant,
                            "alpha": result.test_result.alpha
                        },
                        "effect_size": {
                            "effect_size_type": result.effect_size.effect_size_type.value,
                            "value": result.effect_size.value,
                            "interpretation": result.effect_size.interpretation,
                            "magnitude": result.effect_size.magnitude
                        },
                        "clinical_significance": result.clinical_significance,
                        "summary": result.summary
                    }
                    for result in self.analysis_result.metrics_results
                ]
            }
        }
    
    def to_text_report(self) -> str:
        """ì‚¬ëŒì´ ì½ê¸° ì‰¬ìš´ í…ìŠ¤íŠ¸ ë³´ê³ ì„œ ìƒì„±"""
        report_lines = []
        
        # í—¤ë”
        report_lines.append("=" * 80)
        report_lines.append("í†µê³„ ë¶„ì„ ì¢…í•© ë³´ê³ ì„œ")
        report_lines.append("=" * 80)
        report_lines.append(f"ë³´ê³ ì„œ ID: {self.report_id}")
        report_lines.append(f"ìƒì„± ì‹œê°„: {self.generated_at.strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append("")
        
        # ìš”ì•½ í†µê³„
        report_lines.append("ğŸ“Š ìš”ì•½ í†µê³„")
        report_lines.append("-" * 40)
        report_lines.append(f"ì´ ë©”íŠ¸ë¦­ ìˆ˜: {self.summary_statistics['total_metrics']}")
        report_lines.append(f"í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ë©”íŠ¸ë¦­: {self.summary_statistics['significant_metrics']}")
        report_lines.append(f"ì„ìƒì ìœ¼ë¡œ ìœ ì˜í•œ ë©”íŠ¸ë¦­: {self.summary_statistics['clinically_significant_metrics']}")
        report_lines.append(f"ìœ ì˜ìœ¨: {self.summary_statistics['significance_rate']:.1%}")
        report_lines.append(f"ì„ìƒì  ìœ ì˜ìœ¨: {self.summary_statistics['clinical_significance_rate']:.1%}")
        report_lines.append("")
        
        # Pass/Fail í‰ê°€
        report_lines.append("âœ… Pass/Fail í‰ê°€")
        report_lines.append("-" * 40)
        report_lines.append(f"ì „ì²´ í‰ê°€: {self.pass_fail_assessment['overall_result']}")
        report_lines.append(f"í‰ê°€ ê·¼ê±°: {self.pass_fail_assessment['reasoning']}")
        report_lines.append(f"ì‹ ë¢°ë„: {self.pass_fail_assessment['confidence']}")
        report_lines.append("")
        
        # ë©”íŠ¸ë¦­ë³„ ìƒì„¸ ê²°ê³¼
        report_lines.append("ğŸ“ˆ ë©”íŠ¸ë¦­ë³„ ìƒì„¸ ê²°ê³¼")
        report_lines.append("-" * 40)
        for i, result in enumerate(self.analysis_result.metrics_results, 1):
            report_lines.append(f"{i}. {result.metric_name}")
            report_lines.append(f"   - ê²€ì • ë°©ë²•: {result.test_result.test_type.value}")
            report_lines.append(f"   - í†µê³„ëŸ‰: {result.test_result.statistic:.4f}")
            report_lines.append(f"   - p-value: {result.test_result.p_value:.4f}")
            report_lines.append(f"   - í†µê³„ì  ìœ ì˜ì„±: {'ì˜ˆ' if result.test_result.is_significant else 'ì•„ë‹ˆì˜¤'}")
            report_lines.append(f"   - íš¨ê³¼ í¬ê¸°: {result.effect_size.value:.4f} ({result.effect_size.magnitude})")
            report_lines.append(f"   - ì„ìƒì  ìœ ì˜ì„±: {'ì˜ˆ' if result.clinical_significance else 'ì•„ë‹ˆì˜¤'}")
            report_lines.append(f"   - ìš”ì•½: {result.summary}")
            report_lines.append("")
        
        # ê¶Œì¥ì‚¬í•­
        if self.recommendations:
            report_lines.append("ğŸ’¡ ê¶Œì¥ì‚¬í•­")
            report_lines.append("-" * 40)
            for i, recommendation in enumerate(self.recommendations, 1):
                report_lines.append(f"{i}. {recommendation}")
            report_lines.append("")
        
        # ìœ„í—˜ë„ í‰ê°€
        report_lines.append("âš ï¸ ìœ„í—˜ë„ í‰ê°€")
        report_lines.append("-" * 40)
        report_lines.append(f"ì „ì²´ ìœ„í—˜ë„: {self.risk_assessment['overall_risk']}")
        report_lines.append(f"ìœ„í—˜ ìš”ì†Œ: {', '.join(self.risk_assessment['risk_factors'])}")
        report_lines.append(f"ìœ„í—˜ ì„¤ëª…: {self.risk_assessment['risk_explanation']}")
        report_lines.append("")
        
        # í‘¸í„°
        report_lines.append("=" * 80)
        report_lines.append("ë³´ê³ ì„œ ë")
        report_lines.append("=" * 80)
        
        return "\n".join(report_lines)

class StatisticalAnalysisEngine:
    """í†µê³„ ë¶„ì„ ì—”ì§„"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        StatisticalAnalysisEngine ì´ˆê¸°í™”
        
        Args:
            config: ì„¤ì • íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬
        """
        self.config = config or self._get_default_config()
        logger.info("StatisticalAnalysisEngine ì´ˆê¸°í™” ì™„ë£Œ")
        
    def _get_default_config(self) -> Dict[str, Any]:
        """ê¸°ë³¸ ì„¤ì • ë°˜í™˜"""
        return {
            # í†µê³„ ê²€ì • ì„¤ì •
            'alpha': 0.05,  # ìœ ì˜ìˆ˜ì¤€
            'normality_threshold': 0.05,  # ì •ê·œì„± ê²€ì • ì„ê³„ê°’
            'homogeneity_threshold': 0.05,  # ë“±ë¶„ì‚°ì„± ê²€ì • ì„ê³„ê°’
            
            # íš¨ê³¼ í¬ê¸° ê¸°ì¤€
            'cohens_d_thresholds': {
                'small': 0.2,
                'medium': 0.5,
                'large': 0.8
            },
            
            # ì„ìƒì  ìœ ì˜ì„± ê¸°ì¤€
            'clinical_significance_threshold': 0.5,  # ì¤‘ê°„ ì´ìƒì˜ íš¨ê³¼ í¬ê¸°
            
            # ë°ì´í„° í’ˆì§ˆ ê¸°ì¤€
            'min_sample_size': 10,  # ìµœì†Œ ìƒ˜í”Œ í¬ê¸°
            'max_missing_ratio': 0.3,  # ìµœëŒ€ ê²°ì¸¡ì¹˜ ë¹„ìœ¨
        }
    
    def analyze_periods_comparison(self, 
                                 period_n_data: pd.DataFrame,
                                 period_n1_data: pd.DataFrame,
                                 metrics: List[str] = None) -> ComprehensiveAnalysisResult:
        """
        ë‘ í…ŒìŠ¤íŠ¸ ê¸°ê°„ ê°„ì˜ í†µê³„ì  ë¹„êµ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            period_n_data: í˜„ì¬ ê¸°ê°„(n) ë°ì´í„°
            period_n1_data: ì´ì „ ê¸°ê°„(n-1) ë°ì´í„°
            metrics: ë¶„ì„í•  ë©”íŠ¸ë¦­ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ëª¨ë“  ìˆ«ì ì»¬ëŸ¼ ì‚¬ìš©)
            
        Returns:
            ì¢…í•© ë¶„ì„ ê²°ê³¼
        """
        logger.info("í†µê³„ì  ë¹„êµ ë¶„ì„ ì‹œì‘")
        
        # ë©”íŠ¸ë¦­ ë¦¬ìŠ¤íŠ¸ ê²°ì •
        if metrics is None:
            metrics = self._get_numeric_metrics(period_n_data)
        
        # ê¸°ê°„ ì •ë³´ ìˆ˜ì§‘
        period_n_info = self._extract_period_info(period_n_data, "n")
        period_n1_info = self._extract_period_info(period_n1_data, "n-1")
        
        # ê° ë©”íŠ¸ë¦­ë³„ ë¶„ì„ ìˆ˜í–‰
        metrics_results = []
        significant_count = 0
        clinically_significant_count = 0
        
        for metric in metrics:
            try:
                result = self._analyze_single_metric(
                    period_n_data, period_n1_data, metric
                )
                metrics_results.append(result)
                
                if result.test_result.is_significant:
                    significant_count += 1
                if result.clinical_significance:
                    clinically_significant_count += 1
                    
            except Exception as e:
                logger.error(f"ë©”íŠ¸ë¦­ '{metric}' ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                continue
        
        # ì¢…í•© í‰ê°€ ìƒì„±
        overall_assessment = self._generate_overall_assessment(
            len(metrics_results), significant_count, clinically_significant_count
        )
        
        # ì‹ ë¢°ë„ ê³„ì‚°
        confidence_level = self._calculate_confidence_level(metrics_results)
        
        # ë¶„ì„ ê²°ê³¼ ìƒì„±
        analysis_result = ComprehensiveAnalysisResult(
            analysis_id=f"analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            period_n_info=period_n_info,
            period_n1_info=period_n1_info,
            metrics_results=metrics_results,
            overall_assessment=overall_assessment,
            confidence_level=confidence_level,
            timestamp=datetime.now(),
            total_metrics=len(metrics_results),
            significant_metrics=significant_count,
            clinically_significant_metrics=clinically_significant_count
        )
        
        logger.info(f"í†µê³„ì  ë¹„êµ ë¶„ì„ ì™„ë£Œ: {len(metrics_results)}ê°œ ë©”íŠ¸ë¦­, "
                   f"{significant_count}ê°œ ìœ ì˜, {clinically_significant_count}ê°œ ì„ìƒì  ìœ ì˜")
        
        return analysis_result
    
    def _analyze_single_metric(self, 
                             period_n_data: pd.DataFrame,
                             period_n1_data: pd.DataFrame,
                             metric: str) -> MetricAnalysisResult:
        """
        ë‹¨ì¼ ë©”íŠ¸ë¦­ì— ëŒ€í•œ í†µê³„ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            period_n_data: í˜„ì¬ ê¸°ê°„ ë°ì´í„°
            period_n1_data: ì´ì „ ê¸°ê°„ ë°ì´í„°
            metric: ë¶„ì„í•  ë©”íŠ¸ë¦­
            
        Returns:
            ë©”íŠ¸ë¦­ë³„ ë¶„ì„ ê²°ê³¼
        """
        # ë°ì´í„° ì¶”ì¶œ ë° ì „ì²˜ë¦¬
        n_data = self._extract_metric_data(period_n_data, metric)
        n1_data = self._extract_metric_data(period_n1_data, metric)
        
        # ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬
        if not self._check_data_quality(n_data, n1_data):
            raise ValueError(f"ë©”íŠ¸ë¦­ '{metric}'ì˜ ë°ì´í„° í’ˆì§ˆì´ ê¸°ì¤€ì„ ë§Œì¡±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
        # ì ì ˆí•œ í†µê³„ ê²€ì • ì„ íƒ ë° ìˆ˜í–‰
        test_result = self._perform_statistical_test(n_data, n1_data)
        
        # íš¨ê³¼ í¬ê¸° ê³„ì‚°
        effect_size = self._calculate_effect_size(n_data, n1_data)
        
        # ì„ìƒì  ìœ ì˜ì„± íŒë‹¨
        clinical_significance = self._assess_clinical_significance(test_result, effect_size)
        
        # ìš”ì•½ ìƒì„±
        summary = self._generate_metric_summary(metric, test_result, effect_size, clinical_significance)
        
        return MetricAnalysisResult(
            metric_name=metric,
            period_n_data=n_data,
            period_n1_data=n1_data,
            test_result=test_result,
            effect_size=effect_size,
            clinical_significance=clinical_significance,
            summary=summary
        )
    
    def _extract_metric_data(self, data: pd.DataFrame, metric: str) -> np.ndarray:
        """ë©”íŠ¸ë¦­ ë°ì´í„° ì¶”ì¶œ ë° ì „ì²˜ë¦¬"""
        if metric not in data.columns:
            raise ValueError(f"ë©”íŠ¸ë¦­ '{metric}'ì´ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤.")
        
        # ê²°ì¸¡ì¹˜ ì œê±°
        metric_data = data[metric].dropna().values
        
        if len(metric_data) < self.config['min_sample_size']:
            raise ValueError(f"ë©”íŠ¸ë¦­ '{metric}'ì˜ ìœ íš¨í•œ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
        
        return metric_data
    
    def _check_data_quality(self, n_data: np.ndarray, n1_data: np.ndarray) -> bool:
        """ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬"""
        # ìµœì†Œ ìƒ˜í”Œ í¬ê¸° ê²€ì‚¬
        if len(n_data) < self.config['min_sample_size'] or len(n1_data) < self.config['min_sample_size']:
            return False
        
        # ê²°ì¸¡ì¹˜ ë¹„ìœ¨ ê²€ì‚¬
        n_missing_ratio = np.sum(np.isnan(n_data)) / len(n_data)
        n1_missing_ratio = np.sum(np.isnan(n1_data)) / len(n1_data)
        
        if n_missing_ratio > self.config['max_missing_ratio'] or n1_missing_ratio > self.config['max_missing_ratio']:
            return False
        
        return True
    
    def _perform_statistical_test(self, n_data: np.ndarray, n1_data: np.ndarray) -> StatisticalTestResult:
        """
        ì ì ˆí•œ í†µê³„ ê²€ì •ì„ ì„ íƒí•˜ê³  ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            n_data: í˜„ì¬ ê¸°ê°„ ë°ì´í„°
            n1_data: ì´ì „ ê¸°ê°„ ë°ì´í„°
            
        Returns:
            í†µê³„ ê²€ì • ê²°ê³¼
        """
        # ì •ê·œì„± ê²€ì •
        n_normal = self._test_normality(n_data)
        n1_normal = self._test_normality(n1_data)
        
        # ë“±ë¶„ì‚°ì„± ê²€ì • (ì •ê·œë¶„í¬ì¸ ê²½ìš°ì—ë§Œ)
        equal_variance = True
        if n_normal and n1_normal:
            equal_variance = self._test_homogeneity(n_data, n1_data)
        
        # ì ì ˆí•œ ê²€ì • ë°©ë²• ì„ íƒ ë° ìˆ˜í–‰
        if n_normal and n1_normal and equal_variance:
            # ì •ê·œë¶„í¬ì´ê³  ë“±ë¶„ì‚°ì¸ ê²½ìš°: Student's t-test
            test_type = TestType.T_TEST
            statistic, p_value = self._perform_students_t_test(n_data, n1_data)
        elif n_normal and n1_normal:
            # ì •ê·œë¶„í¬ì´ì§€ë§Œ ë“±ë¶„ì‚°ì´ ì•„ë‹Œ ê²½ìš°: Welch's t-test
            test_type = TestType.T_TEST
            statistic, p_value = self._perform_welchs_t_test(n_data, n1_data)
        else:
            # ë¹„ì •ê·œë¶„í¬ì¸ ê²½ìš°: Mann-Whitney U test
            test_type = TestType.MANN_WHITNEY
            statistic, p_value = self._perform_mann_whitney_test(n_data, n1_data)
        
        return StatisticalTestResult(
            test_type=test_type,
            statistic=statistic,
            p_value=p_value,
            is_significant=p_value < self.config['alpha'],
            alpha=self.config['alpha']
        )
    
    def _perform_students_t_test(self, n_data: np.ndarray, n1_data: np.ndarray) -> Tuple[float, float]:
        """
        Student's t-test ìˆ˜í–‰ (ë“±ë¶„ì‚° ê°€ì •)
        
        Args:
            n_data: í˜„ì¬ ê¸°ê°„ ë°ì´í„°
            n1_data: ì´ì „ ê¸°ê°„ ë°ì´í„°
            
        Returns:
            (í†µê³„ëŸ‰, p-value) íŠœí”Œ
        """
        try:
            statistic, p_value = ttest_ind(n_data, n1_data, equal_var=True)
            logger.debug(f"Student's t-test: statistic={statistic:.4f}, p-value={p_value:.4f}")
            return statistic, p_value
        except Exception as e:
            logger.error(f"Student's t-test ìˆ˜í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            raise
    
    def _perform_welchs_t_test(self, n_data: np.ndarray, n1_data: np.ndarray) -> Tuple[float, float]:
        """
        Welch's t-test ìˆ˜í–‰ (ë¹„ë“±ë¶„ì‚° ê°€ì •)
        
        Args:
            n_data: í˜„ì¬ ê¸°ê°„ ë°ì´í„°
            n1_data: ì´ì „ ê¸°ê°„ ë°ì´í„°
            
        Returns:
            (í†µê³„ëŸ‰, p-value) íŠœí”Œ
        """
        try:
            statistic, p_value = ttest_ind(n_data, n1_data, equal_var=False)
            logger.debug(f"Welch's t-test: statistic={statistic:.4f}, p-value={p_value:.4f}")
            return statistic, p_value
        except Exception as e:
            logger.error(f"Welch's t-test ìˆ˜í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            raise
    
    def _perform_mann_whitney_test(self, n_data: np.ndarray, n1_data: np.ndarray) -> Tuple[float, float]:
        """
        Mann-Whitney U test ìˆ˜í–‰ (ë¹„ëª¨ìˆ˜ ê²€ì •)
        
        Args:
            n_data: í˜„ì¬ ê¸°ê°„ ë°ì´í„°
            n1_data: ì´ì „ ê¸°ê°„ ë°ì´í„°
            
        Returns:
            (í†µê³„ëŸ‰, p-value) íŠœí”Œ
        """
        try:
            statistic, p_value = mannwhitneyu(n_data, n1_data, alternative='two-sided')
            logger.debug(f"Mann-Whitney U test: statistic={statistic:.4f}, p-value={p_value:.4f}")
            return statistic, p_value
        except Exception as e:
            logger.error(f"Mann-Whitney U test ìˆ˜í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            raise
    
    def _perform_wilcoxon_test(self, n_data: np.ndarray, n1_data: np.ndarray) -> Tuple[float, float]:
        """
        Wilcoxon signed-rank test ìˆ˜í–‰ (ëŒ€ì‘í‘œë³¸)
        
        Args:
            n_data: í˜„ì¬ ê¸°ê°„ ë°ì´í„°
            n1_data: ì´ì „ ê¸°ê°„ ë°ì´í„°
            
        Returns:
            (í†µê³„ëŸ‰, p-value) íŠœí”Œ
        """
        try:
            # ë°ì´í„° ê¸¸ì´ê°€ ë‹¤ë¥¸ ê²½ìš° ì§§ì€ ê¸¸ì´ì— ë§ì¶¤
            min_length = min(len(n_data), len(n1_data))
            n_data_trimmed = n_data[:min_length]
            n1_data_trimmed = n1_data[:min_length]
            
            statistic, p_value = wilcoxon(n_data_trimmed, n1_data_trimmed, alternative='two-sided')
            logger.debug(f"Wilcoxon signed-rank test: statistic={statistic:.4f}, p-value={p_value:.4f}")
            return statistic, p_value
        except Exception as e:
            logger.error(f"Wilcoxon signed-rank test ìˆ˜í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            raise
    
    def _perform_paired_t_test(self, n_data: np.ndarray, n1_data: np.ndarray) -> Tuple[float, float]:
        """
        Paired t-test ìˆ˜í–‰ (ëŒ€ì‘í‘œë³¸)
        
        Args:
            n_data: í˜„ì¬ ê¸°ê°„ ë°ì´í„°
            n1_data: ì´ì „ ê¸°ê°„ ë°ì´í„°
            
        Returns:
            (í†µê³„ëŸ‰, p-value) íŠœí”Œ
        """
        try:
            # ë°ì´í„° ê¸¸ì´ê°€ ë‹¤ë¥¸ ê²½ìš° ì§§ì€ ê¸¸ì´ì— ë§ì¶¤
            min_length = min(len(n_data), len(n1_data))
            n_data_trimmed = n_data[:min_length]
            n1_data_trimmed = n1_data[:min_length]
            
            statistic, p_value = stats.ttest_rel(n_data_trimmed, n1_data_trimmed)
            logger.debug(f"Paired t-test: statistic={statistic:.4f}, p-value={p_value:.4f}")
            return statistic, p_value
        except Exception as e:
            logger.error(f"Paired t-test ìˆ˜í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            raise
    
    def _perform_kruskal_wallis_test(self, n_data: np.ndarray, n1_data: np.ndarray) -> Tuple[float, float]:
        """
        Kruskal-Wallis H test ìˆ˜í–‰ (ë¹„ëª¨ìˆ˜ ì¼ì›ë°°ì¹˜ ë¶„ì‚°ë¶„ì„)
        
        Args:
            n_data: í˜„ì¬ ê¸°ê°„ ë°ì´í„°
            n1_data: ì´ì „ ê¸°ê°„ ë°ì´í„°
            
        Returns:
            (í†µê³„ëŸ‰, p-value) íŠœí”Œ
        """
        try:
            statistic, p_value = stats.kruskal(n_data, n1_data)
            logger.debug(f"Kruskal-Wallis H test: statistic={statistic:.4f}, p-value={p_value:.4f}")
            return statistic, p_value
        except Exception as e:
            logger.error(f"Kruskal-Wallis H test ìˆ˜í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            raise
    
    def _perform_anova_test(self, n_data: np.ndarray, n1_data: np.ndarray) -> Tuple[float, float]:
        """
        One-way ANOVA ìˆ˜í–‰ (ì¼ì›ë°°ì¹˜ ë¶„ì‚°ë¶„ì„)
        
        Args:
            n_data: í˜„ì¬ ê¸°ê°„ ë°ì´í„°
            n1_data: ì´ì „ ê¸°ê°„ ë°ì´í„°
            
        Returns:
            (í†µê³„ëŸ‰, p-value) íŠœí”Œ
        """
        try:
            statistic, p_value = stats.f_oneway(n_data, n1_data)
            logger.debug(f"One-way ANOVA: statistic={statistic:.4f}, p-value={p_value:.4f}")
            return statistic, p_value
        except Exception as e:
            logger.error(f"One-way ANOVA ìˆ˜í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            raise
    
    def _perform_ks_test(self, n_data: np.ndarray, n1_data: np.ndarray) -> Tuple[float, float]:
        """
        Kolmogorov-Smirnov test ìˆ˜í–‰ (ë¶„í¬ ë¹„êµ)
        
        Args:
            n_data: í˜„ì¬ ê¸°ê°„ ë°ì´í„°
            n1_data: ì´ì „ ê¸°ê°„ ë°ì´í„°
            
        Returns:
            (í†µê³„ëŸ‰, p-value) íŠœí”Œ
        """
        try:
            statistic, p_value = stats.ks_2samp(n_data, n1_data)
            logger.debug(f"Kolmogorov-Smirnov test: statistic={statistic:.4f}, p-value={p_value:.4f}")
            return statistic, p_value
        except Exception as e:
            logger.error(f"Kolmogorov-Smirnov test ìˆ˜í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            raise
    
    def perform_custom_statistical_test(self, 
                                      n_data: np.ndarray, 
                                      n1_data: np.ndarray, 
                                      test_type: TestType) -> StatisticalTestResult:
        """
        ì‚¬ìš©ìê°€ ì§€ì •í•œ í†µê³„ ê²€ì •ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            n_data: í˜„ì¬ ê¸°ê°„ ë°ì´í„°
            n1_data: ì´ì „ ê¸°ê°„ ë°ì´í„°
            test_type: ìˆ˜í–‰í•  ê²€ì • ìœ í˜•
            
        Returns:
            í†µê³„ ê²€ì • ê²°ê³¼
        """
        try:
            if test_type == TestType.T_TEST:
                # ì •ê·œì„± ê²€ì • í›„ ì ì ˆí•œ t-test ì„ íƒ
                n_normal = self._test_normality(n_data)
                n1_normal = self._test_normality(n1_data)
                
                if n_normal and n1_normal:
                    equal_variance = self._test_homogeneity(n_data, n1_data)
                    if equal_variance:
                        statistic, p_value = self._perform_students_t_test(n_data, n1_data)
                    else:
                        statistic, p_value = self._perform_welchs_t_test(n_data, n1_data)
                else:
                    logger.warning("ì •ê·œì„± ê²€ì • ì‹¤íŒ¨ë¡œ Mann-Whitney U testë¡œ ëŒ€ì²´")
                    statistic, p_value = self._perform_mann_whitney_test(n_data, n1_data)
                    
            elif test_type == TestType.MANN_WHITNEY:
                statistic, p_value = self._perform_mann_whitney_test(n_data, n1_data)
                
            elif test_type == TestType.WILCOXON:
                statistic, p_value = self._perform_wilcoxon_test(n_data, n1_data)
                
            elif test_type == TestType.PAIRED_T_TEST:
                statistic, p_value = self._perform_paired_t_test(n_data, n1_data)
                
            elif test_type == TestType.KRUSKAL_WALLIS:
                statistic, p_value = self._perform_kruskal_wallis_test(n_data, n1_data)
                
            elif test_type == TestType.ANOVA:
                statistic, p_value = self._perform_anova_test(n_data, n1_data)
                
            elif test_type == TestType.KS_TEST:
                statistic, p_value = self._perform_ks_test(n_data, n1_data)
                
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ê²€ì • ìœ í˜•: {test_type}")
            
            return StatisticalTestResult(
                test_type=test_type,
                statistic=statistic,
                p_value=p_value,
                is_significant=p_value < self.config['alpha'],
                alpha=self.config['alpha']
            )
            
        except Exception as e:
            logger.error(f"ì‚¬ìš©ì ì§€ì • í†µê³„ ê²€ì • ìˆ˜í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            raise
    
    def perform_multiple_tests(self, 
                             n_data: np.ndarray, 
                             n1_data: np.ndarray, 
                             test_types: List[TestType]) -> Dict[TestType, StatisticalTestResult]:
        """
        ì—¬ëŸ¬ í†µê³„ ê²€ì •ì„ ë™ì‹œì— ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            n_data: í˜„ì¬ ê¸°ê°„ ë°ì´í„°
            n1_data: ì´ì „ ê¸°ê°„ ë°ì´í„°
            test_types: ìˆ˜í–‰í•  ê²€ì • ìœ í˜• ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ê²€ì • ìœ í˜•ë³„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        results = {}
        
        for test_type in test_types:
            try:
                result = self.perform_custom_statistical_test(n_data, n1_data, test_type)
                results[test_type] = result
                logger.info(f"{test_type.value} ê²€ì • ì™„ë£Œ: p-value={result.p_value:.4f}")
            except Exception as e:
                logger.error(f"{test_type.value} ê²€ì • ì‹¤íŒ¨: {str(e)}")
                continue
        
        return results
    
    def get_recommended_tests(self, n_data: np.ndarray, n1_data: np.ndarray) -> List[TestType]:
        """
        ë°ì´í„° íŠ¹ì„±ì— ë”°ë¼ ê¶Œì¥ë˜ëŠ” í†µê³„ ê²€ì •ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Args:
            n_data: í˜„ì¬ ê¸°ê°„ ë°ì´í„°
            n1_data: ì´ì „ ê¸°ê°„ ë°ì´í„°
            
        Returns:
            ê¶Œì¥ ê²€ì • ìœ í˜• ë¦¬ìŠ¤íŠ¸
        """
        recommended_tests = []
        
        # ì •ê·œì„± ê²€ì •
        n_normal = self._test_normality(n_data)
        n1_normal = self._test_normality(n1_data)
        
        # ë°ì´í„° ê¸¸ì´ í™•ì¸
        n_length = len(n_data)
        n1_length = len(n1_data)
        min_length = min(n_length, n1_length)
        
        if n_normal and n1_normal:
            # ì •ê·œë¶„í¬ì¸ ê²½ìš°
            recommended_tests.append(TestType.T_TEST)
            recommended_tests.append(TestType.ANOVA)
            
            # ëŒ€ì‘í‘œë³¸ ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
            if n_length == n1_length:
                recommended_tests.append(TestType.PAIRED_T_TEST)
                recommended_tests.append(TestType.WILCOXON)
        else:
            # ë¹„ì •ê·œë¶„í¬ì¸ ê²½ìš°
            recommended_tests.append(TestType.MANN_WHITNEY)
            recommended_tests.append(TestType.KRUSKAL_WALLIS)
            
            # ëŒ€ì‘í‘œë³¸ ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
            if n_length == n1_length:
                recommended_tests.append(TestType.WILCOXON)
        
        # ë¶„í¬ ë¹„êµ ê²€ì •ì€ í•­ìƒ ì¶”ê°€
        recommended_tests.append(TestType.KS_TEST)
        
        # ìµœì†Œ ìƒ˜í”Œ í¬ê¸° í™•ì¸
        if min_length < 10:
            logger.warning(f"ìƒ˜í”Œ í¬ê¸°ê°€ ì‘ìŠµë‹ˆë‹¤ ({min_length}). ë¹„ëª¨ìˆ˜ ê²€ì •ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
            # ì‘ì€ ìƒ˜í”Œ í¬ê¸°ì—ì„œëŠ” ë¹„ëª¨ìˆ˜ ê²€ì •ë§Œ ìœ ì§€
            recommended_tests = [test for test in recommended_tests 
                               if test in [TestType.MANN_WHITNEY, TestType.WILCOXON, TestType.KS_TEST]]
        
        return recommended_tests
    
    def analyze_periods_comparison_with_custom_tests(self, 
                                                   period_n_data: pd.DataFrame,
                                                   period_n1_data: pd.DataFrame,
                                                   metrics: List[str] = None,
                                                   test_types: List[TestType] = None) -> ComprehensiveAnalysisResult:
        """
        ì‚¬ìš©ìê°€ ì§€ì •í•œ í†µê³„ ê²€ì •ì„ ì‚¬ìš©í•˜ì—¬ ë‘ í…ŒìŠ¤íŠ¸ ê¸°ê°„ ê°„ì˜ ë¹„êµ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            period_n_data: í˜„ì¬ ê¸°ê°„(n) ë°ì´í„°
            period_n1_data: ì´ì „ ê¸°ê°„(n-1) ë°ì´í„°
            metrics: ë¶„ì„í•  ë©”íŠ¸ë¦­ ë¦¬ìŠ¤íŠ¸
            test_types: ì‚¬ìš©í•  ê²€ì • ìœ í˜• ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ì¢…í•© ë¶„ì„ ê²°ê³¼
        """
        logger.info("ì‚¬ìš©ì ì§€ì • ê²€ì •ì„ ì‚¬ìš©í•œ í†µê³„ì  ë¹„êµ ë¶„ì„ ì‹œì‘")
        
        # ë©”íŠ¸ë¦­ ë¦¬ìŠ¤íŠ¸ ê²°ì •
        if metrics is None:
            metrics = self._get_numeric_metrics(period_n_data)
        
        # ê¸°ê°„ ì •ë³´ ìˆ˜ì§‘
        period_n_info = self._extract_period_info(period_n_data, "n")
        period_n1_info = self._extract_period_info(period_n1_data, "n-1")
        
        # ê° ë©”íŠ¸ë¦­ë³„ ë¶„ì„ ìˆ˜í–‰
        metrics_results = []
        significant_count = 0
        clinically_significant_count = 0
        
        for metric in metrics:
            try:
                # ë°ì´í„° ì¶”ì¶œ ë° ì „ì²˜ë¦¬
                n_data = self._extract_metric_data(period_n_data, metric)
                n1_data = self._extract_metric_data(period_n1_data, metric)
                
                # ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬
                if not self._check_data_quality(n_data, n1_data):
                    logger.warning(f"ë©”íŠ¸ë¦­ '{metric}'ì˜ ë°ì´í„° í’ˆì§ˆì´ ê¸°ì¤€ì„ ë§Œì¡±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                    continue
                
                # ì—¬ëŸ¬ ê²€ì • ìˆ˜í–‰
                test_results = self.perform_multiple_tests(n_data, n1_data, test_types)
                
                # ê°€ì¥ ìœ ì˜í•œ ê²°ê³¼ ì„ íƒ (ê°€ì¥ ì‘ì€ p-value)
                best_result = min(test_results.values(), key=lambda x: x.p_value)
                
                # íš¨ê³¼ í¬ê¸° ê³„ì‚°
                effect_size = self._calculate_effect_size(n_data, n1_data)
                
                # ì„ìƒì  ìœ ì˜ì„± íŒë‹¨
                clinical_significance = self._assess_clinical_significance(best_result, effect_size)
                
                # ìš”ì•½ ìƒì„±
                summary = self._generate_metric_summary(metric, best_result, effect_size, clinical_significance)
                
                # ë©”íŠ¸ë¦­ ê²°ê³¼ ìƒì„±
                metric_result = MetricAnalysisResult(
                    metric_name=metric,
                    period_n_data=n_data,
                    period_n1_data=n1_data,
                    test_result=best_result,
                    effect_size=effect_size,
                    clinical_significance=clinical_significance,
                    summary=summary
                )
                
                metrics_results.append(metric_result)
                
                if best_result.is_significant:
                    significant_count += 1
                if clinical_significance:
                    clinically_significant_count += 1
                    
            except Exception as e:
                logger.error(f"ë©”íŠ¸ë¦­ '{metric}' ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                continue
        
        # ì¢…í•© í‰ê°€ ìƒì„±
        overall_assessment = self._generate_overall_assessment(
            len(metrics_results), significant_count, clinically_significant_count
        )
        
        # ì‹ ë¢°ë„ ê³„ì‚°
        confidence_level = self._calculate_confidence_level(metrics_results)
        
        # ë¶„ì„ ê²°ê³¼ ìƒì„±
        analysis_result = ComprehensiveAnalysisResult(
            analysis_id=f"analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            period_n_info=period_n_info,
            period_n1_info=period_n1_info,
            metrics_results=metrics_results,
            overall_assessment=overall_assessment,
            confidence_level=confidence_level,
            timestamp=datetime.now(),
            total_metrics=len(metrics_results),
            significant_metrics=significant_count,
            clinically_significant_metrics=clinically_significant_count
        )
        
        logger.info(f"ì‚¬ìš©ì ì§€ì • ê²€ì •ì„ ì‚¬ìš©í•œ í†µê³„ì  ë¹„êµ ë¶„ì„ ì™„ë£Œ: {len(metrics_results)}ê°œ ë©”íŠ¸ë¦­, "
                   f"{significant_count}ê°œ ìœ ì˜, {clinically_significant_count}ê°œ ì„ìƒì  ìœ ì˜")
        
        return analysis_result
    
    def _test_normality(self, data: np.ndarray) -> bool:
        """ì •ê·œì„± ê²€ì • (Shapiro-Wilk test)"""
        if len(data) < 3 or len(data) > 5000:  # Shapiro-Wilk test ì œí•œ
            return False
        
        statistic, p_value = shapiro(data)
        return p_value > self.config['normality_threshold']
    
    def _test_homogeneity(self, n_data: np.ndarray, n1_data: np.ndarray) -> bool:
        """ë“±ë¶„ì‚°ì„± ê²€ì • (Levene's test)"""
        statistic, p_value = levene(n_data, n1_data)
        return p_value > self.config['homogeneity_threshold']
    
    def _calculate_effect_size(self, n_data: np.ndarray, n1_data: np.ndarray) -> EffectSizeResult:
        """íš¨ê³¼ í¬ê¸° ê³„ì‚° (Cohen's d)"""
        # Cohen's d ê³„ì‚°
        pooled_std = np.sqrt(((len(n_data) - 1) * np.var(n_data, ddof=1) + 
                             (len(n1_data) - 1) * np.var(n1_data, ddof=1)) / 
                            (len(n_data) + len(n1_data) - 2))
        
        cohens_d = (np.mean(n_data) - np.mean(n1_data)) / pooled_std
        
        # íš¨ê³¼ í¬ê¸° í•´ì„
        magnitude = self._interpret_effect_size(abs(cohens_d))
        
        return EffectSizeResult(
            effect_size_type=EffectSizeType.COHENS_D,
            value=cohens_d,
            interpretation=self._get_effect_size_interpretation(cohens_d),
            magnitude=magnitude
        )
    
    def calculate_comprehensive_effect_sizes(self, n_data: np.ndarray, n1_data: np.ndarray) -> Dict[EffectSizeType, EffectSizeResult]:
        """
        ë‹¤ì–‘í•œ íš¨ê³¼ í¬ê¸°ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        
        Args:
            n_data: í˜„ì¬ ê¸°ê°„ ë°ì´í„°
            n1_data: ì´ì „ ê¸°ê°„ ë°ì´í„°
            
        Returns:
            íš¨ê³¼ í¬ê¸° ìœ í˜•ë³„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        effect_sizes = {}
        
        # Cohen's d
        effect_sizes[EffectSizeType.COHENS_D] = self._calculate_cohens_d(n_data, n1_data)
        
        # Hedges' g
        effect_sizes[EffectSizeType.HEDGES_G] = self._calculate_hedges_g(n_data, n1_data)
        
        # Cliff's Delta
        effect_sizes[EffectSizeType.CLIFFS_DELTA] = self._calculate_cliffs_delta(n_data, n1_data)
        
        return effect_sizes
    
    def _calculate_cohens_d(self, n_data: np.ndarray, n1_data: np.ndarray) -> EffectSizeResult:
        """Cohen's d ê³„ì‚°"""
        # Cohen's d ê³„ì‚°
        pooled_std = np.sqrt(((len(n_data) - 1) * np.var(n_data, ddof=1) + 
                             (len(n1_data) - 1) * np.var(n1_data, ddof=1)) / 
                            (len(n_data) + len(n1_data) - 2))
        
        cohens_d = (np.mean(n_data) - np.mean(n1_data)) / pooled_std
        
        # íš¨ê³¼ í¬ê¸° í•´ì„
        magnitude = self._interpret_effect_size(abs(cohens_d))
        
        return EffectSizeResult(
            effect_size_type=EffectSizeType.COHENS_D,
            value=cohens_d,
            interpretation=self._get_effect_size_interpretation(cohens_d),
            magnitude=magnitude
        )
    
    def _calculate_hedges_g(self, n_data: np.ndarray, n1_data: np.ndarray) -> EffectSizeResult:
        """Hedges' g ê³„ì‚° (Cohen's dì˜ í¸í–¥ ë³´ì • ë²„ì „)"""
        # Cohen's d ê³„ì‚°
        pooled_std = np.sqrt(((len(n_data) - 1) * np.var(n_data, ddof=1) + 
                             (len(n1_data) - 1) * np.var(n1_data, ddof=1)) / 
                            (len(n_data) + len(n1_data) - 2))
        
        cohens_d = (np.mean(n_data) - np.mean(n1_data)) / pooled_std
        
        # Hedges' g ê³„ì‚° (í¸í–¥ ë³´ì •)
        df = len(n_data) + len(n1_data) - 2
        correction_factor = 1 - (3 / (4 * df - 1))
        hedges_g = cohens_d * correction_factor
        
        # íš¨ê³¼ í¬ê¸° í•´ì„
        magnitude = self._interpret_effect_size(abs(hedges_g))
        
        return EffectSizeResult(
            effect_size_type=EffectSizeType.HEDGES_G,
            value=hedges_g,
            interpretation=self._get_effect_size_interpretation(hedges_g),
            magnitude=magnitude
        )
    
    def _calculate_cliffs_delta(self, n_data: np.ndarray, n1_data: np.ndarray) -> EffectSizeResult:
        """Cliff's Delta ê³„ì‚° (ë¹„ëª¨ìˆ˜ íš¨ê³¼ í¬ê¸°)"""
        # ëª¨ë“  ê°€ëŠ¥í•œ ìŒ ë¹„êµ
        wins = 0
        losses = 0
        ties = 0
        total_pairs = len(n_data) * len(n1_data)
        
        for n_val in n_data:
            for n1_val in n1_data:
                if n_val > n1_val:
                    wins += 1
                elif n_val < n1_val:
                    losses += 1
                else:
                    ties += 1
        
        # Cliff's Delta ê³„ì‚°
        cliffs_delta = (wins - losses) / total_pairs
        
        # íš¨ê³¼ í¬ê¸° í•´ì„ (Cliff's Delta ê¸°ì¤€)
        magnitude = self._interpret_cliffs_delta(abs(cliffs_delta))
        
        return EffectSizeResult(
            effect_size_type=EffectSizeType.CLIFFS_DELTA,
            value=cliffs_delta,
            interpretation=self._get_cliffs_delta_interpretation(cliffs_delta),
            magnitude=magnitude
        )
    
    def _interpret_cliffs_delta(self, cliffs_delta: float) -> str:
        """Cliff's Delta í•´ì„"""
        if cliffs_delta < 0.147:
            return 'small'
        elif cliffs_delta < 0.33:
            return 'medium'
        else:
            return 'large'
    
    def _get_cliffs_delta_interpretation(self, cliffs_delta: float) -> str:
        """Cliff's Delta í•´ì„ í…ìŠ¤íŠ¸"""
        magnitude = self._interpret_cliffs_delta(abs(cliffs_delta))
        direction = "ì¦ê°€" if cliffs_delta > 0 else "ê°ì†Œ"
        
        return f"{magnitude} í¬ê¸°ì˜ {direction} íš¨ê³¼ (Cliff's Delta)"
    
    def calculate_confidence_intervals(self, n_data: np.ndarray, n1_data: np.ndarray, 
                                     confidence_level: float = 0.95) -> Dict[str, Dict[str, float]]:
        """
        íš¨ê³¼ í¬ê¸°ì˜ ì‹ ë¢°êµ¬ê°„ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
        
        Args:
            n_data: í˜„ì¬ ê¸°ê°„ ë°ì´í„°
            n1_data: ì´ì „ ê¸°ê°„ ë°ì´í„°
            confidence_level: ì‹ ë¢°ìˆ˜ì¤€ (ê¸°ë³¸ê°’: 0.95)
            
        Returns:
            íš¨ê³¼ í¬ê¸°ë³„ ì‹ ë¢°êµ¬ê°„ ë”•ì…”ë„ˆë¦¬
        """
        intervals = {}
        
        # Cohen's d ì‹ ë¢°êµ¬ê°„
        cohens_d_result = self._calculate_cohens_d(n_data, n1_data)
        cohens_d_ci = self._calculate_cohens_d_confidence_interval(n_data, n1_data, confidence_level)
        intervals['cohens_d'] = {
            'effect_size': cohens_d_result.value,
            'lower_bound': cohens_d_ci[0],
            'upper_bound': cohens_d_ci[1],
            'confidence_level': confidence_level
        }
        
        # Hedges' g ì‹ ë¢°êµ¬ê°„
        hedges_g_result = self._calculate_hedges_g(n_data, n1_data)
        hedges_g_ci = self._calculate_hedges_g_confidence_interval(n_data, n1_data, confidence_level)
        intervals['hedges_g'] = {
            'effect_size': hedges_g_result.value,
            'lower_bound': hedges_g_ci[0],
            'upper_bound': hedges_g_ci[1],
            'confidence_level': confidence_level
        }
        
        return intervals
    
    def _calculate_cohens_d_confidence_interval(self, n_data: np.ndarray, n1_data: np.ndarray, 
                                              confidence_level: float) -> Tuple[float, float]:
        """Cohen's d ì‹ ë¢°êµ¬ê°„ ê³„ì‚°"""
        # Cohen's d ê³„ì‚°
        pooled_std = np.sqrt(((len(n_data) - 1) * np.var(n_data, ddof=1) + 
                             (len(n1_data) - 1) * np.var(n1_data, ddof=1)) / 
                            (len(n_data) + len(n1_data) - 2))
        
        cohens_d = (np.mean(n_data) - np.mean(n1_data)) / pooled_std
        
        # ììœ ë„
        df = len(n_data) + len(n1_data) - 2
        
        # í‘œì¤€ì˜¤ì°¨ ê³„ì‚°
        n1, n2 = len(n_data), len(n1_data)
        se = np.sqrt((n1 + n2) / (n1 * n2) + cohens_d**2 / (2 * (n1 + n2)))
        
        # t ë¶„í¬ì˜ ì„ê³„ê°’
        alpha = 1 - confidence_level
        t_critical = stats.t.ppf(1 - alpha/2, df)
        
        # ì‹ ë¢°êµ¬ê°„
        lower_bound = cohens_d - t_critical * se
        upper_bound = cohens_d + t_critical * se
        
        return lower_bound, upper_bound
    
    def _calculate_hedges_g_confidence_interval(self, n_data: np.ndarray, n1_data: np.ndarray, 
                                              confidence_level: float) -> Tuple[float, float]:
        """Hedges' g ì‹ ë¢°êµ¬ê°„ ê³„ì‚°"""
        # Hedges' g ê³„ì‚°
        hedges_g_result = self._calculate_hedges_g(n_data, n1_data)
        hedges_g = hedges_g_result.value
        
        # ììœ ë„
        df = len(n_data) + len(n1_data) - 2
        
        # í‘œì¤€ì˜¤ì°¨ ê³„ì‚° (Hedges' gìš©)
        n1, n2 = len(n_data), len(n1_data)
        correction_factor = 1 - (3 / (4 * df - 1))
        se = np.sqrt((n1 + n2) / (n1 * n2) + hedges_g**2 / (2 * (n1 + n2))) * correction_factor
        
        # t ë¶„í¬ì˜ ì„ê³„ê°’
        alpha = 1 - confidence_level
        t_critical = stats.t.ppf(1 - alpha/2, df)
        
        # ì‹ ë¢°êµ¬ê°„
        lower_bound = hedges_g - t_critical * se
        upper_bound = hedges_g + t_critical * se
        
        return lower_bound, upper_bound
    
    def _interpret_effect_size(self, effect_size: float) -> str:
        """íš¨ê³¼ í¬ê¸° í•´ì„"""
        thresholds = self.config['cohens_d_thresholds']
        
        if effect_size < thresholds['small']:
            return 'small'
        elif effect_size < thresholds['medium']:
            return 'medium'
        else:
            return 'large'
    
    def _get_effect_size_interpretation(self, effect_size: float) -> str:
        """íš¨ê³¼ í¬ê¸° í•´ì„ í…ìŠ¤íŠ¸"""
        magnitude = self._interpret_effect_size(abs(effect_size))
        direction = "ì¦ê°€" if effect_size > 0 else "ê°ì†Œ"
        
        return f"{magnitude} í¬ê¸°ì˜ {direction} íš¨ê³¼"
    
    def _assess_clinical_significance(self, 
                                    test_result: StatisticalTestResult,
                                    effect_size: EffectSizeResult) -> bool:
        """ì„ìƒì  ìœ ì˜ì„± íŒë‹¨"""
        # í†µê³„ì  ìœ ì˜ì„±ê³¼ íš¨ê³¼ í¬ê¸°ë¥¼ ëª¨ë‘ ê³ ë ¤
        is_statistically_significant = test_result.is_significant
        has_meaningful_effect = effect_size.magnitude in ['medium', 'large']
        
        return is_statistically_significant and has_meaningful_effect
    
    def assess_comprehensive_clinical_significance(self, 
                                                 test_result: StatisticalTestResult,
                                                 effect_sizes: Dict[EffectSizeType, EffectSizeResult],
                                                 n_data: np.ndarray,
                                                 n1_data: np.ndarray) -> Dict[str, Any]:
        """
        ì¢…í•©ì ì¸ ì„ìƒì  ìœ ì˜ì„±ì„ í‰ê°€í•©ë‹ˆë‹¤.
        
        Args:
            test_result: í†µê³„ ê²€ì • ê²°ê³¼
            effect_sizes: ë‹¤ì–‘í•œ íš¨ê³¼ í¬ê¸° ê²°ê³¼
            n_data: í˜„ì¬ ê¸°ê°„ ë°ì´í„°
            n1_data: ì´ì „ ê¸°ê°„ ë°ì´í„°
            
        Returns:
            ì„ìƒì  ìœ ì˜ì„± í‰ê°€ ê²°ê³¼
        """
        assessment = {
            'is_clinically_significant': False,
            'confidence_level': 0.0,
            'reasoning': [],
            'recommendations': [],
            'risk_assessment': 'low'
        }
        
        # 1. í†µê³„ì  ìœ ì˜ì„± í‰ê°€
        if test_result.is_significant:
            assessment['reasoning'].append("í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ì°¨ì´ë¥¼ ë³´ì…ë‹ˆë‹¤.")
            assessment['confidence_level'] += 0.3
        else:
            assessment['reasoning'].append("í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ì°¨ì´ê°€ ì—†ìŠµë‹ˆë‹¤.")
            assessment['recommendations'].append("ë” í° ìƒ˜í”Œ í¬ê¸°ë‚˜ ë‹¤ë¥¸ ê²€ì • ë°©ë²•ì„ ê³ ë ¤í•˜ì„¸ìš”.")
        
        # 2. íš¨ê³¼ í¬ê¸° í‰ê°€
        effect_size_scores = []
        for effect_type, effect_result in effect_sizes.items():
            if effect_result.magnitude == 'large':
                effect_size_scores.append(1.0)
                assessment['reasoning'].append(f"{effect_type.value}: í° íš¨ê³¼ í¬ê¸°")
            elif effect_result.magnitude == 'medium':
                effect_size_scores.append(0.7)
                assessment['reasoning'].append(f"{effect_type.value}: ì¤‘ê°„ íš¨ê³¼ í¬ê¸°")
            else:
                effect_size_scores.append(0.3)
                assessment['reasoning'].append(f"{effect_type.value}: ì‘ì€ íš¨ê³¼ í¬ê¸°")
        
        if effect_size_scores:
            avg_effect_score = np.mean(effect_size_scores)
            assessment['confidence_level'] += avg_effect_score * 0.4
            
            if avg_effect_score >= 0.7:
                assessment['reasoning'].append("ì „ë°˜ì ìœ¼ë¡œ ì˜ë¯¸ ìˆëŠ” íš¨ê³¼ í¬ê¸°ë¥¼ ë³´ì…ë‹ˆë‹¤.")
            elif avg_effect_score >= 0.5:
                assessment['reasoning'].append("ë³´í†µ ìˆ˜ì¤€ì˜ íš¨ê³¼ í¬ê¸°ë¥¼ ë³´ì…ë‹ˆë‹¤.")
            else:
                assessment['reasoning'].append("íš¨ê³¼ í¬ê¸°ê°€ ì‘ìŠµë‹ˆë‹¤.")
        
        # 3. ì‹¤ìš©ì  ì¤‘ìš”ì„± í‰ê°€
        practical_importance = self._assess_practical_importance(n_data, n1_data)
        assessment['confidence_level'] += practical_importance * 0.3
        
        if practical_importance >= 0.7:
            assessment['reasoning'].append("ì‹¤ìš©ì ìœ¼ë¡œ ì¤‘ìš”í•œ ì°¨ì´ë¥¼ ë³´ì…ë‹ˆë‹¤.")
        elif practical_importance >= 0.5:
            assessment['reasoning'].append("ì‹¤ìš©ì  ì¤‘ìš”ì„±ì´ ë³´í†µ ìˆ˜ì¤€ì…ë‹ˆë‹¤.")
        else:
            assessment['reasoning'].append("ì‹¤ìš©ì  ì¤‘ìš”ì„±ì´ ë‚®ìŠµë‹ˆë‹¤.")
        
        # 4. ìµœì¢… ì„ìƒì  ìœ ì˜ì„± íŒë‹¨
        assessment['is_clinically_significant'] = assessment['confidence_level'] >= 0.6
        
        # 5. ìœ„í—˜ë„ í‰ê°€
        assessment['risk_assessment'] = self._assess_risk_level(test_result, effect_sizes, n_data, n1_data)
        
        # 6. ì¶”ê°€ ê¶Œì¥ì‚¬í•­
        if assessment['confidence_level'] < 0.5:
            assessment['recommendations'].append("ì„ìƒì  ìœ ì˜ì„±ì„ í™•ë¦½í•˜ê¸° ìœ„í•´ ì¶”ê°€ ì—°êµ¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        if test_result.p_value < 0.01:
            assessment['recommendations'].append("ë§¤ìš° ê°•í•œ í†µê³„ì  ì¦ê±°ê°€ ìˆìœ¼ë¯€ë¡œ ì„ìƒì  ì ìš©ì„ ê³ ë ¤í•˜ì„¸ìš”.")
        
        return assessment
    
    def _assess_practical_importance(self, n_data: np.ndarray, n1_data: np.ndarray) -> float:
        """ì‹¤ìš©ì  ì¤‘ìš”ì„± í‰ê°€"""
        # í‰ê·  ì°¨ì´ì˜ ì ˆëŒ€ê°’
        mean_diff = abs(np.mean(n_data) - np.mean(n1_data))
        
        # ì „ì²´ ë²”ìœ„ ëŒ€ë¹„ ì°¨ì´ ë¹„ìœ¨
        total_range = max(np.max(n_data), np.max(n1_data)) - min(np.min(n_data), np.min(n1_data))
        if total_range > 0:
            relative_diff = mean_diff / total_range
        else:
            relative_diff = 0
        
        # í‘œì¤€í¸ì°¨ ëŒ€ë¹„ ì°¨ì´
        pooled_std = np.sqrt(((len(n_data) - 1) * np.var(n_data, ddof=1) + 
                             (len(n1_data) - 1) * np.var(n1_data, ddof=1)) / 
                            (len(n_data) + len(n1_data) - 2))
        
        if pooled_std > 0:
            std_diff = mean_diff / pooled_std
        else:
            std_diff = 0
        
        # ì¢…í•© ì ìˆ˜ ê³„ì‚°
        practical_score = (relative_diff * 0.4 + min(std_diff / 2, 1.0) * 0.6)
        
        return min(practical_score, 1.0)
    
    def _assess_risk_level(self, test_result: StatisticalTestResult, 
                          effect_sizes: Dict[EffectSizeType, EffectSizeResult],
                          n_data: np.ndarray, n1_data: np.ndarray) -> str:
        """ìœ„í—˜ë„ í‰ê°€"""
        risk_score = 0
        
        # p-value ê¸°ë°˜ ìœ„í—˜ë„
        if test_result.p_value < 0.001:
            risk_score += 0.1  # ë§¤ìš° ë‚®ì€ ìœ„í—˜
        elif test_result.p_value < 0.01:
            risk_score += 0.2
        elif test_result.p_value < 0.05:
            risk_score += 0.3
        else:
            risk_score += 0.5  # ë†’ì€ ìœ„í—˜
        
        # íš¨ê³¼ í¬ê¸° ê¸°ë°˜ ìœ„í—˜ë„
        effect_size_scores = []
        for effect_result in effect_sizes.values():
            if effect_result.magnitude == 'large':
                effect_size_scores.append(0.1)  # ë‚®ì€ ìœ„í—˜
            elif effect_result.magnitude == 'medium':
                effect_size_scores.append(0.3)
            else:
                effect_size_scores.append(0.5)  # ë†’ì€ ìœ„í—˜
        
        if effect_size_scores:
            avg_effect_risk = np.mean(effect_size_scores)
            risk_score += avg_effect_risk * 0.5
        
        # ìµœì¢… ìœ„í—˜ë„ íŒì •
        if risk_score < 0.3:
            return 'low'
        elif risk_score < 0.6:
            return 'medium'
        else:
            return 'high'
    
    def _generate_metric_summary(self, 
                               metric: str,
                               test_result: StatisticalTestResult,
                               effect_size: EffectSizeResult,
                               clinical_significance: bool) -> str:
        """ë©”íŠ¸ë¦­ë³„ ìš”ì•½ ìƒì„±"""
        significance_text = "ìœ ì˜í•¨" if test_result.is_significant else "ìœ ì˜í•˜ì§€ ì•ŠìŒ"
        clinical_text = "ì„ìƒì ìœ¼ë¡œ ìœ ì˜í•¨" if clinical_significance else "ì„ìƒì ìœ¼ë¡œ ìœ ì˜í•˜ì§€ ì•ŠìŒ"
        
        summary = (f"ë©”íŠ¸ë¦­ '{metric}': {significance_text} (p={test_result.p_value:.4f}), "
                  f"íš¨ê³¼ í¬ê¸°: {effect_size.interpretation} (d={effect_size.value:.3f}), "
                  f"{clinical_text}")
        
        return summary
    
    def _generate_overall_assessment(self, 
                                   total_metrics: int,
                                   significant_metrics: int,
                                   clinically_significant_metrics: int) -> str:
        """ì¢…í•© í‰ê°€ ìƒì„±"""
        if total_metrics == 0:
            return "ë¶„ì„í•  ë©”íŠ¸ë¦­ì´ ì—†ìŠµë‹ˆë‹¤."
        
        significant_ratio = significant_metrics / total_metrics
        clinical_ratio = clinically_significant_metrics / total_metrics
        
        if clinical_ratio >= 0.7:
            assessment = "ë§¤ìš° ì¢‹ìŒ"
        elif clinical_ratio >= 0.5:
            assessment = "ì¢‹ìŒ"
        elif clinical_ratio >= 0.3:
            assessment = "ë³´í†µ"
        else:
            assessment = "ê°œì„  í•„ìš”"
        
        return (f"ì „ì²´ {total_metrics}ê°œ ë©”íŠ¸ë¦­ ì¤‘ {significant_metrics}ê°œ ìœ ì˜ "
                f"({significant_ratio:.1%}), {clinically_significant_metrics}ê°œ ì„ìƒì  ìœ ì˜ "
                f"({clinical_ratio:.1%}). ì¢…í•© í‰ê°€: {assessment}")
    
    def _calculate_confidence_level(self, metrics_results: List[MetricAnalysisResult]) -> float:
        """ì‹ ë¢°ë„ ê³„ì‚°"""
        if not metrics_results:
            return 0.0
        
        # ê° ë©”íŠ¸ë¦­ì˜ ì‹ ë¢°ë„ ê³„ì‚° (p-value ê¸°ë°˜)
        confidences = []
        for result in metrics_results:
            # p-valueë¥¼ ì‹ ë¢°ë„ë¡œ ë³€í™˜ (1 - p_value)
            confidence = 1 - result.test_result.p_value
            confidences.append(confidence)
        
        # í‰ê·  ì‹ ë¢°ë„ ë°˜í™˜
        return np.mean(confidences)
    
    def _get_numeric_metrics(self, data: pd.DataFrame) -> List[str]:
        """ìˆ«ìí˜• ë©”íŠ¸ë¦­ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
        numeric_columns = data.select_dtypes(include=[np.number]).columns.tolist()
        # timestamp ì»¬ëŸ¼ ì œì™¸
        return [col for col in numeric_columns if 'timestamp' not in col.lower()]
    
    def _extract_period_info(self, data: pd.DataFrame, period_name: str) -> Dict[str, Any]:
        """ê¸°ê°„ ì •ë³´ ì¶”ì¶œ"""
        return {
            'period_name': period_name,
            'start_time': data.index.min() if len(data) > 0 else None,
            'end_time': data.index.max() if len(data) > 0 else None,
            'duration_minutes': (data.index.max() - data.index.min()).total_seconds() / 60 if len(data) > 1 else 0,
            'total_records': len(data),
            'metrics_count': len(self._get_numeric_metrics(data))
        }

    def generate_integrated_analysis_report(self, 
                                          analysis_result: ComprehensiveAnalysisResult,
                                          report_format: str = "both") -> Union[IntegratedAnalysisReport, Dict[str, Any], str]:
        """
        í†µí•© ë¶„ì„ ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            analysis_result: ì¢…í•© ë¶„ì„ ê²°ê³¼
            report_format: ë³´ê³ ì„œ í˜•ì‹ ("json", "text", "both")
            
        Returns:
            ë³´ê³ ì„œ ê°ì²´ ë˜ëŠ” JSON/í…ìŠ¤íŠ¸ í˜•íƒœì˜ ë³´ê³ ì„œ
        """
        # ìš”ì•½ í†µê³„ ê³„ì‚°
        summary_stats = self._calculate_summary_statistics(analysis_result)
        
        # Pass/Fail í‰ê°€
        pass_fail_assessment = self._assess_overall_pass_fail(analysis_result)
        
        # ê¶Œì¥ì‚¬í•­ ìƒì„±
        recommendations = self._generate_recommendations(analysis_result)
        
        # ìœ„í—˜ë„ í‰ê°€
        risk_assessment = self._assess_overall_risk(analysis_result)
        
        # ë³´ê³ ì„œ ìƒì„±
        report = IntegratedAnalysisReport(
            report_id=f"report_{analysis_result.analysis_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            analysis_result=analysis_result,
            summary_statistics=summary_stats,
            pass_fail_assessment=pass_fail_assessment,
            recommendations=recommendations,
            risk_assessment=risk_assessment,
            generated_at=datetime.now()
        )
        
        # ìš”ì²­ëœ í˜•ì‹ì— ë”°ë¼ ë°˜í™˜
        if report_format == "json":
            return report.to_json()
        elif report_format == "text":
            return report.to_text_report()
        else:  # "both"
            return report
    
    def _calculate_summary_statistics(self, analysis_result: ComprehensiveAnalysisResult) -> Dict[str, Any]:
        """ìš”ì•½ í†µê³„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        total_metrics = analysis_result.total_metrics
        significant_metrics = analysis_result.significant_metrics
        clinically_significant_metrics = analysis_result.clinically_significant_metrics
        
        # ìœ ì˜ìœ¨ ê³„ì‚°
        significance_rate = significant_metrics / total_metrics if total_metrics > 0 else 0
        clinical_significance_rate = clinically_significant_metrics / total_metrics if total_metrics > 0 else 0
        
        # íš¨ê³¼ í¬ê¸° í†µê³„
        effect_sizes = []
        test_types = []
        p_values = []
        
        for result in analysis_result.metrics_results:
            effect_sizes.append(result.effect_size.value)
            test_types.append(result.test_result.test_type.value)
            p_values.append(result.test_result.p_value)
        
        return {
            "total_metrics": total_metrics,
            "significant_metrics": significant_metrics,
            "clinically_significant_metrics": clinically_significant_metrics,
            "significance_rate": significance_rate,
            "clinical_significance_rate": clinical_significance_rate,
            "avg_effect_size": np.mean(effect_sizes) if effect_sizes else 0,
            "median_effect_size": np.median(effect_sizes) if effect_sizes else 0,
            "min_effect_size": np.min(effect_sizes) if effect_sizes else 0,
            "max_effect_size": np.max(effect_sizes) if effect_sizes else 0,
            "avg_p_value": np.mean(p_values) if p_values else 1,
            "test_type_distribution": {test_type: test_types.count(test_type) for test_type in set(test_types)}
        }
    
    def _assess_overall_pass_fail(self, analysis_result: ComprehensiveAnalysisResult) -> Dict[str, Any]:
        """ì „ì²´ Pass/Fail í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        total_metrics = analysis_result.total_metrics
        significant_metrics = analysis_result.significant_metrics
        clinically_significant_metrics = analysis_result.clinically_significant_metrics
        
        # ê¸°ë³¸ ê¸°ì¤€: 80% ì´ìƒì˜ ë©”íŠ¸ë¦­ì´ í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•˜ê³  ì„ìƒì ìœ¼ë¡œ ìœ ì˜í•´ì•¼ Pass
        significance_threshold = 0.8
        clinical_threshold = 0.8
        
        significance_rate = significant_metrics / total_metrics if total_metrics > 0 else 0
        clinical_rate = clinically_significant_metrics / total_metrics if total_metrics > 0 else 0
        
        # Pass/Fail íŒì •
        if significance_rate >= significance_threshold and clinical_rate >= clinical_threshold:
            overall_result = "PASS"
            confidence = "high"
        elif significance_rate >= 0.6 and clinical_rate >= 0.6:
            overall_result = "CONDITIONAL_PASS"
            confidence = "medium"
        else:
            overall_result = "FAIL"
            confidence = "high"
        
        # í‰ê°€ ê·¼ê±° ìƒì„±
        reasoning = []
        if significance_rate >= significance_threshold:
            reasoning.append(f"í†µê³„ì  ìœ ì˜ì„± ê¸°ì¤€ ì¶©ì¡± ({significance_rate:.1%} >= {significance_threshold:.1%})")
        else:
            reasoning.append(f"í†µê³„ì  ìœ ì˜ì„± ê¸°ì¤€ ë¯¸ì¶©ì¡± ({significance_rate:.1%} < {significance_threshold:.1%})")
        
        if clinical_rate >= clinical_threshold:
            reasoning.append(f"ì„ìƒì  ìœ ì˜ì„± ê¸°ì¤€ ì¶©ì¡± ({clinical_rate:.1%} >= {clinical_threshold:.1%})")
        else:
            reasoning.append(f"ì„ìƒì  ìœ ì˜ì„± ê¸°ì¤€ ë¯¸ì¶©ì¡± ({clinical_rate:.1%} < {clinical_threshold:.1%})")
        
        return {
            "overall_result": overall_result,
            "confidence": confidence,
            "reasoning": " | ".join(reasoning),
            "significance_rate": significance_rate,
            "clinical_rate": clinical_rate,
            "significance_threshold": significance_threshold,
            "clinical_threshold": clinical_threshold
        }
    
    def _generate_recommendations(self, analysis_result: ComprehensiveAnalysisResult) -> List[str]:
        """ê¶Œì¥ì‚¬í•­ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        recommendations = []
        
        total_metrics = analysis_result.total_metrics
        significant_metrics = analysis_result.significant_metrics
        clinically_significant_metrics = analysis_result.clinically_significant_metrics
        
        # í†µê³„ì  ìœ ì˜ì„± ê´€ë ¨ ê¶Œì¥ì‚¬í•­
        significance_rate = significant_metrics / total_metrics if total_metrics > 0 else 0
        if significance_rate < 0.5:
            recommendations.append("í†µê³„ì  ìœ ì˜ì„±ì´ ë‚®ìŠµë‹ˆë‹¤. ìƒ˜í”Œ í¬ê¸°ë¥¼ ëŠ˜ë¦¬ê±°ë‚˜ ì¸¡ì • ë°©ë²•ì„ ê°œì„ í•˜ëŠ” ê²ƒì„ ê³ ë ¤í•˜ì„¸ìš”.")
        elif significance_rate < 0.8:
            recommendations.append("í†µê³„ì  ìœ ì˜ì„±ì´ ë³´í†µ ìˆ˜ì¤€ì…ë‹ˆë‹¤. ì¶”ê°€ ë°ì´í„° ìˆ˜ì§‘ì„ í†µí•´ ê²°ê³¼ë¥¼ ê°•í™”í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
        
        # ì„ìƒì  ìœ ì˜ì„± ê´€ë ¨ ê¶Œì¥ì‚¬í•­
        clinical_rate = clinically_significant_metrics / total_metrics if total_metrics > 0 else 0
        if clinical_rate < 0.5:
            recommendations.append("ì„ìƒì  ìœ ì˜ì„±ì´ ë‚®ìŠµë‹ˆë‹¤. ì‹¤ìš©ì  ì¤‘ìš”ì„±ì„ ë†’ì´ê¸° ìœ„í•œ ê°œì„  ë°©ì•ˆì„ ê²€í† í•˜ì„¸ìš”.")
        elif clinical_rate < 0.8:
            recommendations.append("ì„ìƒì  ìœ ì˜ì„±ì´ ë³´í†µ ìˆ˜ì¤€ì…ë‹ˆë‹¤. íš¨ê³¼ í¬ê¸°ë¥¼ ë†’ì´ê¸° ìœ„í•œ ê°œì…ì„ ê³ ë ¤í•˜ì„¸ìš”.")
        
        # íš¨ê³¼ í¬ê¸° ê´€ë ¨ ê¶Œì¥ì‚¬í•­
        effect_sizes = [result.effect_size.value for result in analysis_result.metrics_results]
        avg_effect_size = np.mean(effect_sizes) if effect_sizes else 0
        
        if avg_effect_size < 0.2:
            recommendations.append("í‰ê·  íš¨ê³¼ í¬ê¸°ê°€ ì‘ìŠµë‹ˆë‹¤. ë” ê°•ë ¥í•œ ê°œì…ì´ë‚˜ ì¸¡ì • ë°©ë²•ì˜ ê°œì„ ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        elif avg_effect_size > 0.8:
            recommendations.append("í‰ê·  íš¨ê³¼ í¬ê¸°ê°€ í½ë‹ˆë‹¤. ê²°ê³¼ì˜ ì•ˆì •ì„±ì„ í™•ì¸í•˜ê¸° ìœ„í•´ ì¶”ê°€ ê²€ì¦ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
        
        # ê²€ì • ë°©ë²• ê´€ë ¨ ê¶Œì¥ì‚¬í•­
        test_types = [result.test_result.test_type.value for result in analysis_result.metrics_results]
        test_distribution = {test_type: test_types.count(test_type) for test_type in set(test_types)}
        
        if len(test_distribution) == 1:
            recommendations.append("ëª¨ë“  ë©”íŠ¸ë¦­ì— ë™ì¼í•œ ê²€ì • ë°©ë²•ì´ ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤. ë°ì´í„° íŠ¹ì„±ì— ë”°ë¥¸ ë‹¤ì–‘í•œ ê²€ì • ë°©ë²• ì ìš©ì„ ê³ ë ¤í•˜ì„¸ìš”.")
        
        # ì¼ë°˜ì ì¸ ê¶Œì¥ì‚¬í•­
        if significant_metrics == total_metrics:
            recommendations.append("ëª¨ë“  ë©”íŠ¸ë¦­ì´ í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•©ë‹ˆë‹¤. ê²°ê³¼ì˜ ì‹ ë¢°ì„±ì„ ë†’ì´ê¸° ìœ„í•´ êµì°¨ ê²€ì¦ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
        
        if len(recommendations) == 0:
            recommendations.append("í˜„ì¬ ê²°ê³¼ëŠ” ì–‘í˜¸í•©ë‹ˆë‹¤. ì •ê¸°ì ì¸ ëª¨ë‹ˆí„°ë§ì„ í†µí•´ ì§€ì†ì ì¸ ê°œì„ ì„ ì¶”ì§„í•˜ì„¸ìš”.")
        
        return recommendations
    
    def _assess_overall_risk(self, analysis_result: ComprehensiveAnalysisResult) -> Dict[str, Any]:
        """ì „ì²´ ìœ„í—˜ë„ë¥¼ í‰ê°€í•©ë‹ˆë‹¤."""
        risk_factors = []
        risk_score = 0
        
        total_metrics = analysis_result.total_metrics
        significant_metrics = analysis_result.significant_metrics
        
        # í†µê³„ì  ìœ ì˜ì„± ê¸°ë°˜ ìœ„í—˜ë„
        significance_rate = significant_metrics / total_metrics if total_metrics > 0 else 0
        if significance_rate < 0.3:
            risk_score += 0.4
            risk_factors.append("ë‚®ì€ í†µê³„ì  ìœ ì˜ì„±")
        elif significance_rate < 0.6:
            risk_score += 0.2
            risk_factors.append("ë³´í†µ í†µê³„ì  ìœ ì˜ì„±")
        
        # p-value ê¸°ë°˜ ìœ„í—˜ë„
        p_values = [result.test_result.p_value for result in analysis_result.metrics_results]
        high_p_values = [p for p in p_values if p > 0.1]
        if len(high_p_values) > len(p_values) * 0.5:
            risk_score += 0.3
            risk_factors.append("ë†’ì€ p-value ë¹„ìœ¨")
        
        # íš¨ê³¼ í¬ê¸° ê¸°ë°˜ ìœ„í—˜ë„
        effect_sizes = [result.effect_size.value for result in analysis_result.metrics_results]
        small_effects = [e for e in effect_sizes if e < 0.2]
        if len(small_effects) > len(effect_sizes) * 0.7:
            risk_score += 0.3
            risk_factors.append("ì‘ì€ íš¨ê³¼ í¬ê¸°")
        
        # ìƒ˜í”Œ í¬ê¸° ê¸°ë°˜ ìœ„í—˜ë„
        for result in analysis_result.metrics_results:
            n_size = len(result.period_n_data)
            n1_size = len(result.period_n1_data)
            if n_size < 30 or n1_size < 30:
                risk_score += 0.1
                risk_factors.append("ì‘ì€ ìƒ˜í”Œ í¬ê¸°")
                break
        
        # ìµœì¢… ìœ„í—˜ë„ íŒì •
        if risk_score < 0.3:
            overall_risk = "low"
            risk_explanation = "ì „ë°˜ì ìœ¼ë¡œ ë‚®ì€ ìœ„í—˜ë„ë¥¼ ë³´ì…ë‹ˆë‹¤."
        elif risk_score < 0.6:
            overall_risk = "medium"
            risk_explanation = "ë³´í†µ ìˆ˜ì¤€ì˜ ìœ„í—˜ë„ë¥¼ ë³´ì…ë‹ˆë‹¤. ì¶”ê°€ ê²€ì¦ì´ ê¶Œì¥ë©ë‹ˆë‹¤."
        else:
            overall_risk = "high"
            risk_explanation = "ë†’ì€ ìœ„í—˜ë„ë¥¼ ë³´ì…ë‹ˆë‹¤. ê²°ê³¼ í•´ì„ì— ì£¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤."
        
        return {
            "overall_risk": overall_risk,
            "risk_score": risk_score,
            "risk_factors": list(set(risk_factors)),  # ì¤‘ë³µ ì œê±°
            "risk_explanation": risk_explanation
        }
    
    def generate_comprehensive_report_with_visualization(self, 
                                                       analysis_result: ComprehensiveAnalysisResult,
                                                       include_charts: bool = True) -> Dict[str, Any]:
        """
        ì‹œê°í™”ë¥¼ í¬í•¨í•œ ì¢…í•© ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            analysis_result: ì¢…í•© ë¶„ì„ ê²°ê³¼
            include_charts: ì°¨íŠ¸ í¬í•¨ ì—¬ë¶€
            
        Returns:
            ì‹œê°í™” ë°ì´í„°ë¥¼ í¬í•¨í•œ ì¢…í•© ë³´ê³ ì„œ
        """
        # ê¸°ë³¸ í†µí•© ë³´ê³ ì„œ ìƒì„±
        integrated_report = self.generate_integrated_analysis_report(analysis_result, "both")
        
        # ì‹œê°í™” ë°ì´í„° ìƒì„±
        visualization_data = {}
        if include_charts:
            visualization_data = self._generate_visualization_data(analysis_result)
        
        # ì¢…í•© ë³´ê³ ì„œ êµ¬ì„±
        comprehensive_report = {
            "report": integrated_report.to_json() if hasattr(integrated_report, 'to_json') else integrated_report,
            "text_report": integrated_report.to_text_report() if hasattr(integrated_report, 'to_text_report') else "",
            "visualization": visualization_data,
            "metadata": {
                "generated_at": datetime.now().isoformat(),
                "analysis_id": analysis_result.analysis_id,
                "total_metrics": analysis_result.total_metrics,
                "confidence_level": analysis_result.confidence_level
            }
        }
        
        return comprehensive_report
    
    def _generate_visualization_data(self, analysis_result: ComprehensiveAnalysisResult) -> Dict[str, Any]:
        """ì‹œê°í™”ë¥¼ ìœ„í•œ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        visualization_data = {}
        
        # ë©”íŠ¸ë¦­ë³„ ê²°ê³¼ ìš”ì•½
        metrics_summary = []
        for result in analysis_result.metrics_results:
            metrics_summary.append({
                "metric_name": result.metric_name,
                "p_value": result.test_result.p_value,
                "effect_size": result.effect_size.value,
                "is_significant": result.test_result.is_significant,
                "clinical_significance": result.clinical_significance,
                "test_type": result.test_result.test_type.value
            })
        
        visualization_data["metrics_summary"] = metrics_summary
        
        # í†µê³„ì  ìœ ì˜ì„± ë¶„í¬
        significant_count = sum(1 for result in analysis_result.metrics_results if result.test_result.is_significant)
        non_significant_count = len(analysis_result.metrics_results) - significant_count
        
        visualization_data["significance_distribution"] = {
            "significant": significant_count,
            "non_significant": non_significant_count,
            "total": len(analysis_result.metrics_results)
        }
        
        # íš¨ê³¼ í¬ê¸° ë¶„í¬
        effect_sizes = [result.effect_size.value for result in analysis_result.metrics_results]
        visualization_data["effect_size_distribution"] = {
            "values": effect_sizes,
            "mean": np.mean(effect_sizes) if effect_sizes else 0,
            "median": np.median(effect_sizes) if effect_sizes else 0,
            "std": np.std(effect_sizes) if effect_sizes else 0
        }
        
        # ê²€ì • ë°©ë²• ë¶„í¬
        test_types = [result.test_result.test_type.value for result in analysis_result.metrics_results]
        test_distribution = {test_type: test_types.count(test_type) for test_type in set(test_types)}
        visualization_data["test_type_distribution"] = test_distribution
        
        # p-value ë¶„í¬
        p_values = [result.test_result.p_value for result in analysis_result.metrics_results]
        visualization_data["p_value_distribution"] = {
            "values": p_values,
            "mean": np.mean(p_values) if p_values else 1,
            "median": np.median(p_values) if p_values else 1,
            "significant_count": sum(1 for p in p_values if p < 0.05)
        }
        
        return visualization_data
